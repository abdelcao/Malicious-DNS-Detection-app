{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGA Domain Detection using Deep Learning\n",
    "## Binary Classification: Legitimate vs DGA-generated Domains\n",
    "\n",
    "This notebook implements a deep learning model using NLP techniques to detect Domain Generation Algorithm (DGA) generated domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file '.venv/lib/python3.10/site-packages/typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "dga_df = pd.read_csv('dga_websites.csv')\n",
    "legit_df = pd.read_csv('legit_websites.csv')\n",
    "\n",
    "print(\"DGA Dataset Shape:\", dga_df.shape)\n",
    "print(\"Legitimate Dataset Shape:\", legit_df.shape)\n",
    "print(\"\\nFirst few DGA domains:\")\n",
    "print(dga_df.head())\n",
    "print(\"\\nFirst few legitimate domains:\")\n",
    "print(legit_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "df = pd.concat([dga_df, legit_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass distribution (%)\")\n",
    "print(df['class'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove any missing values if present\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert class to binary: legit=0, dga=1\n",
    "df['label'] = df['class'].map({'legit': 0, 'dga': 1})\n",
    "\n",
    "print(\"\\nData after preprocessing:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain length analysis\n",
    "df['domain_length'] = df['domain'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[df['class'] == 'legit']['domain_length'].hist(bins=50, alpha=0.7, label='Legitimate', color='green')\n",
    "df[df['class'] == 'dga']['domain_length'].hist(bins=50, alpha=0.7, label='DGA', color='red')\n",
    "plt.xlabel('Domain Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Domain Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.groupby('class')['domain_length'].mean().plot(kind='bar', color=['green', 'red'])\n",
    "plt.xlabel('Domain Type')\n",
    "plt.ylabel('Average Length')\n",
    "plt.title('Average Domain Length by Type')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Domain length statistics:\")\n",
    "print(df.groupby('class')['domain_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character distribution analysis\n",
    "def calculate_entropy(domain):\n",
    "    \"\"\"Calculate Shannon entropy of domain\"\"\"\n",
    "    prob = [float(domain.count(c)) / len(domain) for c in set(domain)]\n",
    "    entropy = -sum([p * np.log2(p) for p in prob])\n",
    "    return entropy\n",
    "\n",
    "df['entropy'] = df['domain'].apply(calculate_entropy)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[df['class'] == 'legit']['entropy'].hist(bins=50, alpha=0.7, label='Legitimate', color='green')\n",
    "df[df['class'] == 'dga']['entropy'].hist(bins=50, alpha=0.7, label='DGA', color='red')\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Domain Entropy Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df, x='class', y='entropy', palette={'legit': 'green', 'dga': 'red'})\n",
    "plt.title('Entropy by Domain Type')\n",
    "plt.xlabel('Domain Type')\n",
    "plt.ylabel('Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenization (treating each character as a token)\n",
    "def char_tokenize(domain):\n",
    "    \"\"\"Convert domain to space-separated characters\"\"\"\n",
    "    return ' '.join(list(domain))\n",
    "\n",
    "df['char_tokens'] = df['domain'].apply(char_tokenize)\n",
    "\n",
    "print(\"Example of character tokenization:\")\n",
    "print(f\"Original: {df['domain'].iloc[0]}\")\n",
    "print(f\"Tokenized: {df['char_tokens'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = df['char_tokens'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_features = 50  # Number of unique characters (a-z, 0-9, special chars)\n",
    "max_length = 75    # Maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Training data shape: {X_train_pad.shape}\")\n",
    "print(f\"Testing data shape: {X_test_pad.shape}\")\n",
    "print(f\"\\nExample sequence (first domain):\")\n",
    "print(f\"Original: {X_train[0]}\")\n",
    "print(f\"Sequence: {X_train_seq[0][:20]}...\")  # Show first 20 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture: Hybrid CNN-LSTM\n",
    "def build_dga_detector(vocab_size, max_length, embedding_dim=64):\n",
    "    \"\"\"\n",
    "    Build a hybrid CNN-LSTM model for DGA detection\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer for character representation\n",
    "    2. Conv1D layers for local pattern detection\n",
    "    3. Bidirectional LSTM for sequential dependencies\n",
    "    4. Dense layers for classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        \n",
    "        # Convolutional layers for n-gram feature extraction\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Bidirectional LSTM for sequence learning\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = build_dga_detector(vocab_size, max_length)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), \n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_dga_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 1].set_title('Model Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[1, 0].plot(history.history['auc'], label='Train AUC')\n",
    "axes[1, 0].plot(history.history['val_auc'], label='Validation AUC')\n",
    "axes[1, 0].set_title('Model AUC')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('AUC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Precision and Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
    "axes[1, 1].set_title('Precision and Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test AUC:       {test_auc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall:    {test_recall:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_pad, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'DGA']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'DGA'], \n",
    "            yticklabels=['Legitimate', 'DGA'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives:  {cm[0][0]}\")\n",
    "print(f\"False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]}\")\n",
    "print(f\"True Positives:  {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Model with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_domain(domain, model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Predict if a domain is legitimate or DGA-generated\n",
    "    \"\"\"\n",
    "    # Tokenize domain\n",
    "    char_tokens = ' '.join(list(domain))\n",
    "    sequence = tokenizer.texts_to_sequences([char_tokens])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded, verbose=0)[0][0]\n",
    "    \n",
    "    # Interpret result\n",
    "    label = \"DGA\" if prediction > 0.5 else \"Legitimate\"\n",
    "    confidence = prediction if prediction > 0.5 else (1 - prediction)\n",
    "    \n",
    "    return label, confidence, prediction\n",
    "\n",
    "# Test with sample domains\n",
    "test_domains = [\n",
    "    \"google\",\n",
    "    \"facebook\",\n",
    "    \"amazon\",\n",
    "    \"xjfkdslfjkdslfj\",\n",
    "    \"qwertyasdfgh\",\n",
    "    \"microsoft\",\n",
    "    \"apple\",\n",
    "    \"zgxcnmvbnmcvb\",\n",
    "    \"youtube\",\n",
    "    \"github\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Domain':<25} {'Prediction':<15} {'Confidence':<15} {'Score':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for domain in test_domains:\n",
    "    label, confidence, score = predict_domain(domain, model, tokenizer, max_length)\n",
    "    print(f\"{domain:<25} {label:<15} {confidence*100:>6.2f}%         {score:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('../model/dga_detector_model.h5')\n",
    "print(\"✓ Model saved as 'dga_detector_model.h5'\")\n",
    "\n",
    "# Save model in SavedModel format (for TensorFlow Serving)\n",
    "model.save('../model/dga_detector_model_savedmodel', save_format='tf')\n",
    "print(\"✓ Model saved in SavedModel format\")\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('../model/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"✓ Tokenizer saved as 'tokenizer.pkl'\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'max_length': max_length,\n",
    "    'vocab_size': vocab_size,\n",
    "    'threshold': 0.5\n",
    "}\n",
    "\n",
    "with open('../model/model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "print(\"✓ Model configuration saved as 'model_config.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All model artifacts saved successfully!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - dga_detector_model.h5 (Keras model)\")\n",
    "print(\"  - dga_detector_model_savedmodel/ (TensorFlow SavedModel)\")\n",
    "print(\"  - tokenizer.pkl (Character tokenizer)\")\n",
    "print(\"  - model_config.pkl (Model configuration)\")\n",
    "print(\"  - best_dga_model.h5 (Best checkpoint)\")\n",
    "print(\"  - training_history.png (Training plots)\")\n",
    "print(\"  - confusion_matrix.png (Confusion matrix)\")\n",
    "print(\"  - roc_curve.png (ROC curve)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Architecture: Hybrid CNN-LSTM\")\n",
    "print(f\"Total Parameters: {model.count_params():,}\")\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  - Total samples: {len(df):,}\")\n",
    "print(f\"  - Training samples: {len(X_train):,}\")\n",
    "print(f\"  - Testing samples: {len(X_test):,}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  - Test AUC: {test_auc:.4f}\")\n",
    "print(f\"  - Test Precision: {test_precision:.4f}\")\n",
    "print(f\"  - Test Recall: {test_recall:.4f}\")\n",
    "print(f\"\\nKey Features:\")\n",
    "print(f\"  - Character-level NLP tokenization\")\n",
    "print(f\"  - Convolutional layers for pattern detection\")\n",
    "print(f\"  - Bidirectional LSTM for sequence learning\")\n",
    "print(f\"  - Dropout regularization to prevent overfitting\")\n",
    "print(f\"  - Early stopping and learning rate scheduling\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Model is ready for deployment!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Malicious-DNS-Detection-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
